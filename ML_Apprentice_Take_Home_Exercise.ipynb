{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albert Zhang - ML Apprentice Take Home Exercise\n",
    "## Sentence Transformers & Multi-Task Learning\n",
    "---\n",
    "This notebook is structured to complete the following tasks:\n",
    "- **Task 1**: Sentence Transformer Implementation\n",
    "- **Task 2**: Multi-Task Learning Expansion\n",
    "- **Task 3**: Training Considerations & Transfer Learning\n",
    "- **Task 4**: Multi-Task Learning Training Loop (Bonus)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation\n",
    "We use HuggingFace Transformers to implement a sentence transformer. The model will encode input sentences into fixed-length embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alber\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class SentenceTransformer(torch.nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        tokens = self.tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.transformer(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "# Example usage\n",
    "model = SentenceTransformer()\n",
    "embeddings = model([\"This is a test sentence.\", \"I love machine learning!.\"])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion\n",
    "We expand our model to support two tasks:\n",
    "- **Task A**: Sentence Classification\n",
    "- **Task B**: Sentiment Analysis \n",
    "\n",
    "This is done by adding two linear task-specific heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(SentenceTransformer):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes_a=3, num_classes_b=2):\n",
    "        super(MultiTaskModel, self).__init__(model_name)\n",
    "        self.classifier_a = torch.nn.Linear(self.transformer.config.hidden_size, num_classes_a)\n",
    "        self.classifier_b = torch.nn.Linear(self.transformer.config.hidden_size, num_classes_b)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeddings = super().forward(sentences)\n",
    "        return {\n",
    "            'task_a': self.classifier_a(embeddings),\n",
    "            'task_b': self.classifier_b(embeddings)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Training Considerations\n",
    "Scenarios discussed:\n",
    "- **Freezing entire network**: Useful for inference with minimal resources.\n",
    "- **Freezing backbone only**: Enables quick adaptation via fine-tuning task-specific heads.\n",
    "- **Freezing task-specific heads**: Can help preserve specific outputs during multi-stage training.\n",
    "\n",
    "### Transfer Learning Approach:\n",
    "- Pre-trained Model: `distilbert-base-uncased`\n",
    "- Frozen Layers: First few transformer layers or entire transformer for faster convergence\n",
    "- Trainable Layers: Task-specific heads for domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Multi-Task Training Loop (Bonus)\n",
    "We define a simple training loop using synthetic data and illustrate metric tracking and loss handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_loop(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        sentences, labels_a, labels_b = batch\n",
    "        outputs = model(sentences)\n",
    "        \n",
    "        loss_a = F.cross_entropy(outputs['task_a'], labels_a)\n",
    "        loss_b = F.cross_entropy(outputs['task_b'], labels_b)\n",
    "        loss = loss_a + loss_b\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
